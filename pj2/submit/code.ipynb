{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It's all about processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsvReader(filepath):\n",
    "    '''read in .tsv files, and leave the sentence ID'''\n",
    "    df = pd.read_csv(filepath, sep=\"\\t\")\n",
    "    # assert sum(sum(pd.isnull(df.values))) == 0     # There's no missing value\n",
    "    return df[['PhraseId','Phrase','Sentiment']] # 只保留phrase ID，句子，label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"./data/train.tsv\"\n",
    "\n",
    "dataset = tsvReader(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156060\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId                                             Phrase  Sentiment\n",
       "0         1  A series of escapades demonstrating the adage ...          1\n",
       "1         2  A series of escapades demonstrating the adage ...          2\n",
       "2         3                                           A series          2\n",
       "3         4                                                  A          2\n",
       "4         5                                             series          2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    79582\n",
      "3    32927\n",
      "1    27273\n",
      "4     9206\n",
      "0     7072\n",
      "Name: Sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "dataset.head()\n",
    "print(dataset['Sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import utils\n",
    "import gensim.parsing.preprocessing as gsp\n",
    "\n",
    "filters = [\n",
    "            gsp.strip_tags,\n",
    "            gsp.strip_punctuation,\n",
    "            gsp.strip_multiple_whitespaces,\n",
    "            gsp.remove_stopwords,   \n",
    "            gsp.strip_short,\n",
    "            gsp.stem_text\n",
    "        ]\n",
    "\n",
    "def clean_text(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = utils.to_unicode(sentence)\n",
    "    for func in filters:\n",
    "        sentence = func(sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['clean_sentence'] = dataset['Phrase'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seri escapad demonstr adag good goos\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>clean_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "      <td>seri escapad demonstr adag good goos good gand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "      <td>seri escapad demonstr adag good goos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "      <td>seri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "      <td>seri</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId                                             Phrase  Sentiment  \\\n",
       "0         1  A series of escapades demonstrating the adage ...          1   \n",
       "1         2  A series of escapades demonstrating the adage ...          2   \n",
       "2         3                                           A series          2   \n",
       "3         4                                                  A          2   \n",
       "4         5                                             series          2   \n",
       "\n",
       "                                      clean_sentence  \n",
       "0  seri escapad demonstr adag good goos good gand...  \n",
       "1               seri escapad demonstr adag good goos  \n",
       "2                                               seri  \n",
       "3                                                     \n",
       "4                                               seri  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset.iloc[1,3])\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Below are analysis borrowed from another kaggle kernel**\n",
    "\n",
    "As was mentioned in the original competition description, there are no individual movie reviews but rather phrases taken out of context and split into smaller parts, each with an assigned sentiment category. The competition is evaluated based on scoring results of each test phrase, so the context of the whole review does not matter here. The data is also fairly clean, so there will not be need for much pre-processing. \n",
    "\n",
    "Before proceeding it is also a good idea to look at distribution of data, to see if the classes in training set are evenly distributed. For that I borrowed code from another github user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset['Sentiment'].value_counts().sort_index().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classes seem to follow a normal distribution, with most frequently distributed class being \"2\". This could lead to model not having sufficient data to learn the less-represented classes. This is something to be aware of when evaluating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the dataset into Training, Developing and Testing Data\n",
    "\n",
    "def train_dev_test_spliter(data_len, train_propor):   \n",
    "    '''split data into train: development: test = 8:1:1 '''\n",
    "    samind1 = set(random.sample(range(data_len), int(data_len*train_propor)))\n",
    "    samind0 = set(range(data_len)) - samind1\n",
    "    samind2 = set(random.sample(samind0, int(len(samind0) * 0.5)))\n",
    "    samind3 = samind0 - samind2\n",
    "    return list(samind1), list(samind2), list(samind3)\n",
    "    \n",
    "def spliter(data, index):\n",
    "    '''split data into train: development: test = len of index[0]:index[1]:index[2]'''\n",
    "    return data.iloc[index[0]], data.iloc[index[1]], data.iloc[index[2]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154499 780 781\n"
     ]
    }
   ],
   "source": [
    "index = train_dev_test_spliter(len(dataset),0.99)\n",
    "\n",
    "train_text, dev_text, test_text = spliter(dataset['clean_sentence'], index)\n",
    "train_label, dev_label, test_label = spliter(dataset['Sentiment'], index)\n",
    "\n",
    "print(len(train_text), len(dev_text), len(test_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use built-in functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "countVector = CountVectorizer(binary=True, min_df=2) \n",
    "# countVector = CountVectorizer(binary=True, max_df=0.95) \n",
    "countVector.fit_transform(train_text.to_list())   \n",
    "trainFeatureSet = countVector.transform(train_text.to_list())\n",
    "testFeatureSet = countVector.transform(test_text.to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature scaling; Not really good performance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler(with_mean=False)\n",
    "sc.fit(trainFeatureSet) \n",
    "trainFeatureSet = sc.transform(trainFeatureSet)\n",
    "testFeatureSet = sc.transform(testFeatureSet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\equee\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logRegModel = LogisticRegression()\n",
    "logRegModel.fit(trainFeatureSet, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate Accuracy with countVectorizer :  63.07830321671152\n"
     ]
    }
   ],
   "source": [
    "predLogReg1 = logRegModel.predict(testFeatureSet)\n",
    "score = logRegModel.score(testFeatureSet, test_label)\n",
    "print(\"Approximate Accuracy with countVectorizer : \", score*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidVector = TfidfVectorizer(min_df=2, norm='l2', smooth_idf=True, use_idf=True)\n",
    "\n",
    "tfidTrainFeatures = tfidVector.fit_transform(train_text)\n",
    "tfidTestFeatures = tfidVector.transform(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature scaling; Not really good performance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler(with_mean=False)\n",
    "sc.fit(trainFeatureSet) \n",
    "trainFeatureSet = sc.transform(trainFeatureSet)\n",
    "testFeatureSet = sc.transform(testFeatureSet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\equee\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logRegModel2 = LogisticRegression()\n",
    "logRegModel2.fit(tfidTrainFeatures, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate Accuracy with Tfidf :  62.546456491093174\n"
     ]
    }
   ],
   "source": [
    "predLogReg2 = logRegModel2.predict(tfidTestFeatures)\n",
    "score2 = logRegModel2.score(tfidTestFeatures, test_label)\n",
    "print(\"Approximate Accuracy with Tfidf : \", score2*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use self-defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting features by counting word counts...\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse import csr_matrix\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import _IS_32BIT\n",
    "import array\n",
    "import numbers\n",
    "from collections.abc import Mapping\n",
    "from functools import partial      \n",
    "from sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "def _document_frequency(X):\n",
    "    '''count the number of non-zero values for each feature in sparse X'''\n",
    "    if sp.isspmatrix_csr(X):\n",
    "        return np.bincount(X.indices, minlength=X.shape[1])\n",
    "    else:\n",
    "        return np.diff(X.indptr) \n",
    "\n",
    "def _make_int_array():\n",
    "    '''construct an array.array of a type suitable for scipy.sparse indice'''\n",
    "    return array.array(str('i'))\n",
    "\n",
    "def _analyze(doc, analyzer=None, tokenizer=None, ngrams=None, preprocessor=None, decoder=None, stop_words=None):\n",
    "    '''chain together an optional series of text preprocessing steps to go from a single document to ngrams'''\n",
    "    if decoder is not None:\n",
    "        doc = decoder(doc)\n",
    "\n",
    "    if analyzer is not None:\n",
    "        doc = analyzer(doc)\n",
    "    \n",
    "    else:\n",
    "        if preprocessor is not None:\n",
    "            doc = preprocessor(doc)\n",
    "        if tokenizer is not None:\n",
    "            doc = tokenizer(doc)\n",
    "        if ngrams is not None:\n",
    "            if stop_words is not None:\n",
    "                doc = ngrams(doc, stop_words)\n",
    "            else:\n",
    "                doc = ngrams(doc)\n",
    "\n",
    "    return doc\n",
    "\n",
    "def _preprocess(doc, accent_function=None, lower=False):\n",
    "    '''chian together an optional series of text preprocessing steps to apply to a document'''\n",
    "    if lower:\n",
    "        doc = doc.lower()\n",
    "    if accent_function is not None:\n",
    "        doc = accent_function(doc)\n",
    "    return doc\n",
    "\n",
    "########### 用except this函数们得到准确率50% ###########\n",
    "def _check_stop_list(stop):\n",
    "    if stop == 'english':\n",
    "        return ENGLISH_STOP_WORDS\n",
    "    elif isinstance(stop, str):\n",
    "        raise ValueError(\"not a built-in stop list: %s\" % stop)\n",
    "    elif stop is None:\n",
    "        return None\n",
    "    else: # assume it's a collection\n",
    "        return frozenset(stop)    \n",
    "########### 用except this函数们得到准确率50% ###########\n",
    "\n",
    "class CountVector():\n",
    "    def __init__(self, *, input='content', encoding='utf-8',\n",
    "                 decode_error='strict', strip_accents=None,\n",
    "                 lowercase=True, preprocessor=None, tokenizer=None,\n",
    "                 stop_words=None, token_pattern=r'(?u)\\b\\w\\w+\\b',\n",
    "                 ngram_range=(1,1), analyzer='word',\n",
    "                 max_df=1.0, min_df=1, max_features=None,\n",
    "                 vocabulary=None, binary=False, dtype=np.int64\n",
    "                 ):\n",
    "        self.input = input\n",
    "        self.encoding = encoding\n",
    "        self.strip_accents = decode_error\n",
    "        self.preprocessor = preprocessor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.analyzer = analyzer\n",
    "        self.lowercase = lowercase\n",
    "        self.token_pattern = token_pattern\n",
    "        self.stop_words = stop_words\n",
    "        self.max_df = max_df\n",
    "        self.min_df = min_df\n",
    "        if max_df < 0 or min_df < 0:\n",
    "            raise ValueError(\"negative value for max_df or min_df\")\n",
    "        self.max_features = max_features\n",
    "        if max_features is not None:\n",
    "            if (not isinstance(max_features, numbers.Integral) or max_features <=0):\n",
    "                raise ValueError('max_features=%r, neigher a positive integer nor None' % max_features)\n",
    "        self.ngram_range = ngram_range\n",
    "        self.vocabulary = vocabulary\n",
    "        self.binary = binary\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def _sort_features(self, X, vocabulary):\n",
    "        '''sort features by name; returns a recorded matrix and modifies the vocab in place'''\n",
    "        sorted_features = sorted(vocabulary.items())\n",
    "        map_index = np.empty(len(sorted_features), dtype=X.indices.dtype)\n",
    "        for new_val, (term, old_val) in enumerate(sorted_features):\n",
    "            vocabulary[term] = new_val\n",
    "            map_index[old_val] = new_val\n",
    "        \n",
    "        X.indices = map_index.take(X.indices, mode='clip')\n",
    "        return X\n",
    "    \n",
    "    def _limit_features(self, X, vocabulary, high=None, low=None, limit=None):\n",
    "        '''remove too rare or too common features'''\n",
    "        if high is None and low is None and limit is None:\n",
    "            return X, set()\n",
    "\n",
    "        # calculate a mask based on document frequencies\n",
    "        dfs = _document_frequency(X)\n",
    "        mask = np.ones(len(dfs), dtype=bool)\n",
    "        if high is not None:\n",
    "            mask &= dfs <= high  \n",
    "        if low is not None:\n",
    "            mask &= dfs >= low\n",
    "        if limit is not None and mask.sum() > limit:\n",
    "            tfs = np.asarray(X.sum(axis=0)).ravel() \n",
    "            mask_inds = (-tfs[mask]).argsort()[:limit]\n",
    "            new_mask = np.zeros(len(dfs), dtype=bool)\n",
    "            new_mask[np.where(mask)[0][mask_inds]] = True\n",
    "            mask = new_mask\n",
    "\n",
    "        new_indices = np.cumsum(mask) - 1       # maps old indices to new\n",
    "        removed_terms = set()\n",
    "        for term, old_index in list(vocabulary.items()):\n",
    "            if mask[old_index]:\n",
    "                vocabulary[term] = new_indices[old_index]\n",
    "            else:\n",
    "                del vocabulary[term]\n",
    "                removed_terms.add(term)\n",
    "        kept_indices = np.where(mask)[0]\n",
    "        if len(kept_indices) == 0:\n",
    "            raise ValueError(\"after pruning, no terms remain. Try a lower min_df or a hight max_df.\")\n",
    "\n",
    "        return X[:,kept_indices], removed_terms\n",
    "\n",
    "    def _count_vocab(self, raw_documents, fixed_vocab):\n",
    "        '''create sparse feature matrix, and vocabulary where fixed_vocab=False'''\n",
    "        if fixed_vocab:\n",
    "            vocabulary = self.vocabulary_\n",
    "        else:\n",
    "            vocabulary = defaultdict()\n",
    "            vocabulary.default_factory = vocabulary.__len__\n",
    "        \n",
    "        analyze = self.build_analyzer()\n",
    "        j_indices = []\n",
    "        indptr = []\n",
    "\n",
    "        values = _make_int_array()\n",
    "        indptr.append(0)\n",
    "        for doc in raw_documents:\n",
    "            feature_counter = {}\n",
    "            for feature in analyze(doc):      \n",
    "                try:\n",
    "                    feature_idx = vocabulary[feature]\n",
    "                    if feature_idx not in feature_counter:\n",
    "                        feature_counter[feature_idx] = 1\n",
    "                    else:\n",
    "                        feature_counter[feature_idx] += 1\n",
    "                except KeyError:\n",
    "                    # NOTE: Ignore out-of-vocabulary items for fixed_vocab=True\n",
    "                    continue\n",
    "            \n",
    "            j_indices.extend(feature_counter.keys())\n",
    "            values.extend(feature_counter.values())\n",
    "            indptr.append(len(j_indices))\n",
    "\n",
    "        if not fixed_vocab:\n",
    "            vocabulary = dict(vocabulary)\n",
    "            if not vocabulary:\n",
    "                raise ValueError(\"empty vocabulary; perhaps the documents contain stop words\")\n",
    "\n",
    "        if indptr[-1] > np.iinfo(np.int32).max:     # 2**31-1\n",
    "            if _IS_32BIT:\n",
    "                raise ValueError(('sparse CSR array has {} non-zero elements and requires 64 bit indexing, which is unsupported with 32 bit Python'.format(indptr[-1])))\n",
    "            indices_dtype = np.int64\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            indices_dtype = np.int32\n",
    "        j_indices = np.asarray(j_indices, dtype = indices_dtype)\n",
    "        indptr = np.asarray(indptr, dtype=indices_dtype)\n",
    "        values = np.frombuffer(values, dtype=np.intc)\n",
    "\n",
    "        X = sp.csr_matrix((values, j_indices, indptr), shape=(len(indptr) - 1, len(vocabulary)),\n",
    "                            dtype=self.dtype)\n",
    "        X.sort_indices()\n",
    "        return vocabulary, X\n",
    "\n",
    "    # def _check_vocabulary(self):\n",
    "    #     '''check if vocabulary is empty or missing'''\n",
    "    #     if not hasattr(self, 'vocabulary_'):\n",
    "    #         self._validate_vocabulary()\n",
    "    #         if not self.fixed_vocabulary_:\n",
    "    #             raise ValueError   \n",
    "        \n",
    "    #     if len(self.vocabulary_) == 0:\n",
    "    #         raise ValueError\n",
    "\n",
    "    \n",
    "    def _validate_vocabulary(self):\n",
    "        '''check if vocabulary is empty or missing'''\n",
    "        vocabulary = self.vocabulary\n",
    "        if vocabulary is not None:\n",
    "            if isinstance(vocabulary, set):\n",
    "                vocabulary = sorted(vocabulary)\n",
    "            if not isinstance(vocabulary, Mapping):\n",
    "                vocab = {}\n",
    "                for i, t in enumerate(vocabulary):\n",
    "                    if vocab.setdefault(t,i) != i:\n",
    "                        msg = \"Duplicate term in vocabulary: %r\" % t\n",
    "                        raise ValueError(msg)\n",
    "                vocabulary = vocab\n",
    "            else:\n",
    "                indices = set(vocabulary.values())\n",
    "                if len(indices) != len(vocabulary):\n",
    "                    raise ValueError\n",
    "                for i in range(len(vocabulary)):\n",
    "                    if i not in indices:\n",
    "                        msg = '...'\n",
    "                        raise ValueError(msg)\n",
    "            \n",
    "            if not vocabulary:\n",
    "                raise ValueError\n",
    "            self.fixed_vocabulary_ = True\n",
    "            self.vocabulary_ = dict(vocabulary)\n",
    "        else:\n",
    "            self.fixed_vocabulary_ = False\n",
    "        \n",
    "    def _validate_params(self):\n",
    "        '''check validity of ngram_range parameter'''\n",
    "        min_n, max_m = self.ngram_range\n",
    "        if min_n > max_m:\n",
    "            raise ValueError\n",
    "        \n",
    "    def build_analyzer(self):\n",
    "        '''return a callable that handles preprocessing, tokenization and n-grams generation'''\n",
    "        if callable(self.analyzer):\n",
    "            return partial(_analyze, analyzer=self.analyzer, decoder=self.decode)\n",
    "        \n",
    "        preprocess = self.build_preprocessor()\n",
    "\n",
    "        if self.analyzer == 'word':\n",
    "            stop_words = self.get_stop_words()\n",
    "            tokenize = self.build_tokenizer()\n",
    "            self._check_stop_words_consistency(stop_words, preprocess, tokenize)\n",
    "            return partial(_analyze, ngrams=self._word_ngrams, tokenizer=tokenize, preprocessor=preprocess, decoder=self.decode, stop_words=stop_words)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError('%s is not a valid tokenization scheme/analyzer' %\n",
    "                             self.analyzer)\n",
    "\n",
    "    \n",
    "    def build_preprocessor(self):\n",
    "        '''return a function to prerpocess the text before tokenization'''\n",
    "        if self.preprocessor is not None:\n",
    "            return self.preprocessor\n",
    "        \n",
    "        # accent stripping\n",
    "        strip_accents = None\n",
    "\n",
    "        return partial(_preprocess, accent_function = strip_accents, lower=self.lowercase)\n",
    "\n",
    "    def _word_ngrams(self, tokens, stop_words=None):\n",
    "        if stop_words is not None:\n",
    "            tokens = [w for w in tokens if w not in stop_words]\n",
    "\n",
    "        # handle token n-grams\n",
    "        min_n, max_n = self.ngram_range\n",
    "        if max_n != 1:\n",
    "            original_tokens = tokens\n",
    "            if min_n == 1:\n",
    "                tokens = list(original_tokens)\n",
    "                min_n += 1\n",
    "            else:\n",
    "                tokens = []\n",
    "\n",
    "            n_original_tokens = len(original_tokens)\n",
    "\n",
    "            # bind method outside of loop to reduce overhead\n",
    "            tokens_append = tokens.append\n",
    "            space_join = \" \".join\n",
    "\n",
    "            for n in range(min_n, min(max_n + 1, n_original_tokens + 1)):\n",
    "                for i in range(n_original_tokens - n + 1):\n",
    "                    tokens_append(space_join(original_tokens[i:i + 1]))\n",
    "                \n",
    "        return tokens\n",
    "\n",
    "    def decode(self, doc):\n",
    "        '''decode the input into a tring of unicode symbols'''\n",
    "        if self.input == 'filename':\n",
    "            with open(doc, 'rb') as fh:\n",
    "                doc = fh.read()\n",
    "            \n",
    "        elif self.input == 'file':\n",
    "            doc = doc.read()\n",
    "\n",
    "        if isinstance(doc, bytes):\n",
    "            doc = doc.decode(self.encoding, self.decode_error)\n",
    "        \n",
    "        if doc is np.nan:\n",
    "            raise ValueError(\"np.nan is an invalid document, expected byte or \"\n",
    "                             \"unicode string.\")\n",
    "                             \n",
    "        return doc\n",
    "\n",
    "    ########### 上述函数得到的结果表现为51.5% ###########\n",
    "    ########### 加上sklearn自己的stopword，tokenizer表现为63% ###########\n",
    "\n",
    "    def build_tokenizer(self):\n",
    "        '''turn a function that splits a string into a sequence of tokens'''\n",
    "        if self.tokenizer is not None:\n",
    "            return self.tokenizer\n",
    "        token_pattern = re.compile(self.token_pattern)\n",
    "\n",
    "        if token_pattern.groups > 1:\n",
    "            raise ValueError(\"More than 1 capturing group in token pattern. Only a single \"\n",
    "                \"group should be captured.\")\n",
    "\n",
    "        return token_pattern.findall\n",
    "\n",
    "    def get_stop_words(self):\n",
    "        '''build or fetch the effective stop words list'''\n",
    "        return _check_stop_list(self.stop_words)\n",
    "\n",
    "    def _check_stop_words_consistency(self, stop_words, preprocess, tokenize):\n",
    "        '''check if stop words are consistent'''\n",
    "        if id(self.stop_words) == getattr(self, '_stop_words_id', None):\n",
    "            # stop words were previously valiated\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            inconsistent = set()\n",
    "            for w in stop_words or ():\n",
    "                tokens = list(tokenize(preprocess(w)))\n",
    "                for token in tokens:\n",
    "                    if token not in stop_words:\n",
    "                        inconsistent.add(token)\n",
    "            self._stop_words_id = id(self.stop_words)\n",
    "\n",
    "            if inconsistent: return not inconsistent    #\n",
    "        \n",
    "        except Exception:\n",
    "            # Failed to check stop words consistency (e.g. because a custom preprocessor or tokenizer was used)\n",
    "            self._stop_words_id = id(self.stop_words)\n",
    "            return 'error'\n",
    "\n",
    "\n",
    "    ########### built-in father class ###########\n",
    "\n",
    "    def fit_transform(self, raw_documents, y=None):\n",
    "        ''' learn the vocabulay dictionary and return document-term'''\n",
    "        if isinstance(raw_documents, str):\n",
    "            raise ValueError\n",
    "        \n",
    "        self._validate_params()\n",
    "        self._validate_vocabulary()\n",
    "        max_df = self.max_df\n",
    "        min_df = self.min_df\n",
    "        max_features = self.max_features\n",
    "\n",
    "        vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n",
    "        \n",
    "        if self.binary:\n",
    "            X.data.fill(1)\n",
    "        \n",
    "        if not self.fixed_vocabulary_:\n",
    "            n_doc = X.shape[0]\n",
    "            max_doc_count = (max_df if isinstance(max_df, numbers.Integral) else max_df * n_doc)\n",
    "            min_doc_count = (min_df if isinstance(min_df, numbers.Integral) else min_df * n_doc)\n",
    "            if max_doc_count < min_doc_count:\n",
    "                raise ValueError\n",
    "            if max_features is not None:\n",
    "                X = self._sort_features(X, vocabulary)\n",
    "            X, self.stop_words_ = self._limit_features(X, vocabulary, max_doc_count, min_doc_count, max_features)\n",
    "            if max_features is None:\n",
    "                X = self._sort_features(X, vocabulary)\n",
    "            self.vocabulary_ = vocabulary\n",
    "        \n",
    "        return X\n",
    "    \n",
    "\n",
    "    def transform(self, raw_documents):\n",
    "        '''transform documents to document-term matrix'''\n",
    "        if isinstance(raw_documents, str):\n",
    "            raise ValueError\n",
    "\n",
    "        # self._check_vocabulary()\n",
    "\n",
    "        # use the same matrix-building strategy as fit_transform\n",
    "        _, X = self._count_vocab(raw_documents, fixed_vocab=True)\n",
    "        if self.binary:\n",
    "            X.data.fill(1)\n",
    "        return X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "countVector = CountVector(binary=True, min_df=2)\n",
    "# countVector = CountVectorizer(binary=True, max_df=0.95) \n",
    "trainFeatureSet = countVector.fit_transform(train_text.to_list())\n",
    "testFeatureSet = countVector.transform(test_text.to_list())\n",
    "devFeatureSet = countVector.transform(dev_text.to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate Accuracy with countVectorizer :  61.8437900128041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junhao/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logRegModel = LogisticRegression()\n",
    "logRegModel.fit(trainFeatureSet, train_label)\n",
    "\n",
    "predLogReg1 = logRegModel.predict(testFeatureSet)\n",
    "score = logRegModel.score(testFeatureSet, test_label)\n",
    "print(\"Approximate Accuracy with countVectorizer : \", score*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier Funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "classNum = 5\n",
    "\n",
    "\n",
    "class AdaGrad:\n",
    "\tdef __init__(self, num_row, num_col, eta=0.1, epsilon=1e-8):\n",
    "\t\tself.eta = eta\n",
    "\t\tself.epsilon = epsilon\n",
    "\t\tself.cache = np.zeros((num_row, num_col))\n",
    "\n",
    "\tdef update(self, W, gradient):\n",
    "\t\tself.cache += gradient ** 2\n",
    "\t\tW -= self.eta / np.sqrt(self.cache + self.epsilon) * gradient\n",
    "\t\treturn W\n",
    "\n",
    "\n",
    "class SoftmaxRegression:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def softmax(self, x):\n",
    "        # return np.exp(x) / np.sum(np.exp(x))\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / np.sum(e_x)\n",
    "\n",
    "\n",
    "    def train_bgd(self, x_train, y_train, x_dev, y_dev, batch_size=1000, max_itr=100, alpha=0.1, lamda=0, tol=1e-4):\n",
    "        self.weights = np.random.uniform(0, 1, (classNum, x_train.shape[1]+1))\n",
    "        optimizer = AdaGrad(num_row=x_train.shape[1]+1, num_col=classNum)\n",
    "        losses = []\n",
    "        # dev_x = x_dev.toarray()\n",
    "        dev_x = np.hstack((x_dev.toarray(), np.ones((x_dev.shape[0],1))))\n",
    "        for itr in range(max_itr):\n",
    "            err = np.zeros((classNum, x_train.shape[1]+1))\n",
    "            cum_err = 0\n",
    "            \n",
    "            for i in range(0,len(x_train.indptr)-1, batch_size):\n",
    "                cur_batch_size = min(batch_size,len(x_train.indptr)-1-i)\n",
    "                x = np.hstack((x_train[i:i+batch_size].toarray(), np.ones((cur_batch_size,1))))\n",
    "                y = np.zeros((5,cur_batch_size))\n",
    "                for j in range(cur_batch_size):\n",
    "                    y[:,j][y_train.iloc[i+j]] = 1\n",
    "                # h_y = self.softmax(np.dot(self.weights, x.T))\n",
    "                h_y = np.apply_along_axis(self.softmax, 0, np.dot(self.weights, x.T))\n",
    "\n",
    "                errs = np.dot((h_y - y), x)\n",
    "                err = np.sum(errs, axis=1) / cur_batch_size\n",
    "\n",
    "                regular = (alpha * lamda * self.weights) / cur_batch_size\n",
    "                # self.weights = self.weights - alpha * err.reshape((-1,1)) + regular\n",
    "                # self.weights = optimizer.update(self.weights.T, err.T).T\n",
    "                self.weights = optimizer.update(self.weights.T, err.T + regular).T\n",
    "                # self.weights = self.weights - alpha * err.reshape((-1,1))\n",
    "\n",
    "            h_y = np.apply_along_axis(self.softmax, 0, np.dot(self.weights, dev_x.T))\n",
    "            # deviation = [1 - np.log(h_y[:,j][y_dev.iloc[j]]+1e-6) for j in range(len(y_dev))]\n",
    "            deviation = [1 - np.log(h_y[:,j][y_dev.iloc[j]]) for j in range(len(y_dev))]\n",
    "            losses.append(np.sum(deviation) / cur_batch_size)\n",
    "\n",
    "            if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < tol:\n",
    "                print('converge! last_iter:{:d}, last loss:{:.4f}'.format(itr,losses[-1]))\n",
    "                break \n",
    "\n",
    "            print('iter:{:d}, dev_loss:{:.4f}'.format(itr,losses[-1]))\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        y_predict = []\n",
    "        for i in range(len(x_test.indptr)-1):\n",
    "            x = np.hstack((x_test[i].toarray(), np.ones((1,1)))).T\n",
    "            y = np.argmax(np.dot(self.weights, x))\n",
    "            y_predict.append(y)\n",
    "\n",
    "        return y_predict\n",
    "\n",
    "    def test(self, y_test, y_predict):\n",
    "        print(\"testing...\")\n",
    "        accuracy = 0\n",
    "        for i in range(len(y_test)):\n",
    "            if y_predict[i] == y_test.iloc[i]:\n",
    "                accuracy += 1\n",
    "\n",
    "        accuracy = accuracy / len(y_test)\n",
    "        print(\"accuracy = {:.3f}\".format(accuracy))\n",
    "        return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:0, dev_loss:0.5038\n",
      "iter:1, dev_loss:0.5041\n",
      "iter:2, dev_loss:0.5041\n",
      "iter:3, dev_loss:0.5041\n",
      "iter:4, dev_loss:0.5041\n",
      "iter:5, dev_loss:0.5041\n",
      "iter:6, dev_loss:0.5041\n",
      "iter:7, dev_loss:0.5041\n",
      "iter:8, dev_loss:0.5041\n",
      "iter:9, dev_loss:0.5041\n",
      "iter:10, dev_loss:0.5041\n",
      "iter:11, dev_loss:0.5041\n",
      "iter:12, dev_loss:0.5041\n",
      "iter:13, dev_loss:0.5041\n",
      "iter:14, dev_loss:0.5041\n",
      "iter:15, dev_loss:0.5041\n",
      "iter:16, dev_loss:0.5041\n",
      "iter:17, dev_loss:0.5041\n",
      "iter:18, dev_loss:0.5041\n",
      "iter:19, dev_loss:0.5041\n",
      "iter:20, dev_loss:0.5041\n",
      "iter:21, dev_loss:0.5041\n",
      "iter:22, dev_loss:0.5041\n",
      "iter:23, dev_loss:0.5041\n",
      "iter:24, dev_loss:0.5041\n",
      "iter:25, dev_loss:0.5041\n",
      "iter:26, dev_loss:0.5041\n",
      "iter:27, dev_loss:0.5041\n",
      "iter:28, dev_loss:0.5041\n",
      "iter:29, dev_loss:0.5041\n",
      "testing...\n",
      "accuracy = 0.201\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_train_vec, x_test_vec, y_train, y_test = trainFeatureSet, testFeatureSet, train_label, test_label\n",
    "x_dev_vec, y_dev = devFeatureSet, dev_label\n",
    "\n",
    "sl = SoftmaxRegression()\n",
    "sl.train_bgd(x_train_vec, y_train, x_dev_vec, y_dev, batch_size=10000, lamda=0.2, alpha=0.15, max_itr=30, tol=0)\n",
    "y_predict = sl.predict(x_test_vec)\n",
    "accuracy = sl.test(y_test, y_predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "classNum = 5\n",
    "\n",
    "class AdaGrad:\n",
    "\tdef __init__(self, num_row, num_col, eta=0.1, epsilon=1e-8):\n",
    "\t\tself.eta = eta\n",
    "\t\tself.epsilon = epsilon\n",
    "\t\tself.cache = np.zeros((num_row, num_col))\n",
    "\n",
    "\tdef update(self, W, gradient):\n",
    "\t\tself.cache += gradient ** 2\n",
    "\t\tW -= self.eta / np.sqrt(self.cache + self.epsilon) * gradient\n",
    "\t\treturn W\n",
    "        \n",
    "class LogisticRegression:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def logistic(self, x):\n",
    "        temp = 1 / (1 + np.exp(-x))\n",
    "        return temp / np.sum(temp)\n",
    "        \n",
    "\n",
    "    def train_bgd(self, x_train, y_train, x_dev, y_dev, batch_size=1000, max_itr=100, alpha=0.1, lamda=0, tol=1e-4):\n",
    "        self.weights = np.random.uniform(0, 1, (classNum, x_train.shape[1]+1))\n",
    "        optimizer = AdaGrad(num_row=x_train.shape[1]+1, num_col=classNum)\n",
    "        losses = []\n",
    "        dev_x = np.hstack((x_dev.toarray(), np.ones((x_dev.shape[0],1))))\n",
    "        for itr in range(max_itr):\n",
    "            err = np.zeros((classNum, x_train.shape[1]+1))\n",
    "            cum_err = 0\n",
    "            \n",
    "            for i in range(0,len(x_train.indptr)-1, batch_size):\n",
    "                cur_batch_size = min(batch_size,len(x_train.indptr)-1-i)\n",
    "                x = np.hstack((x_train[i:i+batch_size].toarray(), np.ones((cur_batch_size,1))))\n",
    "                y = np.zeros((5,cur_batch_size))\n",
    "                for j in range(cur_batch_size):  y[:,j][y_train.iloc[i+j]] = 1\n",
    "                h_y = np.apply_along_axis(self.logistic, 0, np.dot(self.weights, x.T))\n",
    "\n",
    "                # MSE loss gradient\n",
    "                # h_y = self.logistic(np.dot(self.weights, x.T))\n",
    "                errs = np.dot((h_y - y), x)\n",
    "\n",
    "                # Cross Entropy loss gradient\n",
    "                # h_yy = np.zeros((5,cur_batch_size))\n",
    "                # for j in range(cur_batch_size):  h_yy[y_train.iloc[i+j],j] = h_y[y_train.iloc[i+j],j]\n",
    "                # errs = np.dot((h_yy - y), x)\n",
    "\n",
    "                \n",
    "                err = np.sum(errs, axis=1) / batch_size\n",
    "                regular = (alpha * lamda * self.weights) / batch_size\n",
    "                # self.weights = self.weights - alpha * err.reshape((-1,1)) + regular\n",
    "                # self.weights = optimizer.update(self.weights.T, err.T + regular).T\n",
    "                self.weights = optimizer.update(self.weights.T, err.T).T\n",
    "\n",
    "            h_y = np.apply_along_axis(self.logistic, 0, np.dot(self.weights, dev_x.T))\n",
    "            \n",
    "            # MSE loss\n",
    "            y = np.zeros((5, dev_x.shape[0]))\n",
    "            for j in range(dev_x.shape[0]):  y[:,j][y_dev.iloc[j]] = 1\n",
    "            losses.append(np.sum(np.power(h_y-y,2)) / dev_x.shape[0])\n",
    "\n",
    "            # Cross-Entropy loss\n",
    "            # deviation = [1 - np.log(h_y[:,j][y_dev.iloc[j]]) for j in range(len(y_dev))]\n",
    "            # losses.append(np.sum(deviation) / batch_size)\n",
    "\n",
    "            if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < tol:\n",
    "                print('converge! last_iter:{:d}, last loss:{:.4f}'.format(itr,losses[-1]))\n",
    "                break \n",
    "\n",
    "            print('iter:{:d}, dev_loss:{:.4f}'.format(itr,losses[-1]))\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        y_predict = []\n",
    "        for i in range(len(x_test.indptr)-1):\n",
    "            x = np.hstack((x_test[i].toarray(), np.ones((1,1)))).T\n",
    "            y = np.argmax(np.dot(self.weights, x))\n",
    "            y_predict.append(y)\n",
    "\n",
    "        return y_predict\n",
    "\n",
    "    def test(self, y_test, y_predict):\n",
    "        print(\"testing...\")\n",
    "        accuracy = 0\n",
    "        for i in range(len(y_test)):\n",
    "            if y_predict[i] == y_test.iloc[i]:\n",
    "                accuracy += 1\n",
    "\n",
    "        accuracy = accuracy / len(y_test)\n",
    "        print(\"accuracy = {:.3f}\".format(accuracy))\n",
    "        return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:0, dev_loss:0.7222\n",
      "iter:1, dev_loss:0.6970\n",
      "iter:2, dev_loss:0.6887\n",
      "iter:3, dev_loss:0.6869\n",
      "iter:4, dev_loss:0.6863\n",
      "iter:5, dev_loss:0.6860\n",
      "iter:6, dev_loss:0.6860\n",
      "iter:7, dev_loss:0.6860\n",
      "iter:8, dev_loss:0.6860\n",
      "iter:9, dev_loss:0.6860\n",
      "iter:10, dev_loss:0.6860\n",
      "iter:11, dev_loss:0.6861\n",
      "iter:12, dev_loss:0.6861\n",
      "iter:13, dev_loss:0.6861\n",
      "iter:14, dev_loss:0.6861\n",
      "iter:15, dev_loss:0.6861\n",
      "iter:16, dev_loss:0.6862\n",
      "iter:17, dev_loss:0.6862\n",
      "iter:18, dev_loss:0.6862\n",
      "iter:19, dev_loss:0.6862\n",
      "iter:20, dev_loss:0.6862\n",
      "iter:21, dev_loss:0.6862\n",
      "iter:22, dev_loss:0.6863\n",
      "iter:23, dev_loss:0.6863\n",
      "iter:24, dev_loss:0.6863\n",
      "iter:25, dev_loss:0.6863\n",
      "iter:26, dev_loss:0.6863\n",
      "iter:27, dev_loss:0.6863\n",
      "iter:28, dev_loss:0.6863\n",
      "iter:29, dev_loss:0.6863\n",
      "testing...\n",
      "accuracy = 0.517\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_train_vec, x_test_vec, y_train, y_test = trainFeatureSet, testFeatureSet, train_label, test_label\n",
    "x_dev_vec, y_dev = devFeatureSet, dev_label\n",
    "\n",
    "sl = LogisticRegression()\n",
    "sl.train_bgd(x_train_vec, y_train, x_dev_vec, y_dev, lamda=0.2, alpha=0.1, batch_size=10000, max_itr=30, tol=0)\n",
    "y_predict = sl.predict(x_test_vec)\n",
    "accuracy = sl.test(y_test, y_predict)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c8f9177a0acce8018d23d7772672ff7f2c1807cf103258a4b51e26a443b2e37b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
