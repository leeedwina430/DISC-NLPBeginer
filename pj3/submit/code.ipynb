{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the document online "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget http://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! unzip stanfordSentimentTreebank.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm stanfordSentimentTreebank.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle, glob, random, time\n",
    "import os.path as op\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 0\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data_utils.py\n",
    "\n",
    "deal with data format and negative sample probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StanfordSentiment:\n",
    "    def __init__(self, path=None, tablesize=1000000):\n",
    "        if not path:\n",
    "            # path = \"./stanfordSentimentTreebank\"\n",
    "            path = \"./datasets/stanfordSentimentTreebank\"\n",
    "\n",
    "        self.path = path\n",
    "        self.tablesize = tablesize\n",
    "    \n",
    "    def tokens(self):\n",
    "        '''return the tokens (word:index) mapping\n",
    "           create ind2token mapping (list)\n",
    "           wordcount: total words number; token freq: just each word's count\n",
    "        '''\n",
    "        if hasattr(self, '_tokens') and self._tokens:\n",
    "            return self._tokens\n",
    "\n",
    "        tokens = dict()\n",
    "        tokenfreq = dict()\n",
    "        wordcount = 0\n",
    "        revtokens = []\n",
    "        idx = 0\n",
    "\n",
    "        for sentence in self.sentences():\n",
    "            for w in sentence:\n",
    "                wordcount += 1\n",
    "                if not w in tokens:\n",
    "                    tokens[w] = idx     # NOTE: tokens 从0开始\n",
    "                    revtokens += [w]\n",
    "                    tokenfreq[w] = 1\n",
    "                    idx += 1\n",
    "                else:\n",
    "                    tokenfreq[w] += 1\n",
    "\n",
    "        tokens['UNK'] = idx\n",
    "        revtokens += ['UNK']\n",
    "        tokenfreq[\"UNK\"] = 1\n",
    "        wordcount += 1\n",
    "\n",
    "        self._tokens = tokens\n",
    "        self._tokenfreq = tokenfreq\n",
    "        self._wordcount = wordcount\n",
    "        self._revtokens = revtokens # ind2tokens, 没必要再用一个dic，直接list存储\n",
    "        return self._tokens\n",
    "\n",
    "    def sentences(self):\n",
    "        '''get pure lower case words list for each sentence in the file'''\n",
    "        if hasattr(self, '_sentences') and self._sentences:\n",
    "            return self._sentences\n",
    "\n",
    "        sentences = []\n",
    "        with open(self.path + \"/datasetSentences.txt\",'r') as f:\n",
    "            first = True\n",
    "            for line in f:\n",
    "                if first:\n",
    "                    first = False\n",
    "                    continue    # 第一个不做处理\n",
    "\n",
    "                splitted = line.strip().split()[1:] # 第一个不做处理\n",
    "                # Deal with some peculiar encoding issues with this file\n",
    "                sentences += [[w.lower() for w in splitted]]\n",
    "\n",
    "        self._sentences = sentences\n",
    "        # self._sentlengths = np.array([len(s) for s in sentences])\n",
    "        # self._cumsentlen = np.cumsum(self._sentlengths) # NOTE: ？？为什么记录cumsum\n",
    "\n",
    "        return self._sentences\n",
    "\n",
    "    def numSentences(self):\n",
    "        '''return the number of sentences'''\n",
    "        if hasattr(self, '_numSentences') and self._numSentences:\n",
    "            return self._numSentences\n",
    "\n",
    "        else:\n",
    "            self._numSentences = len(self.sentences())\n",
    "            return self._numSentences\n",
    "\n",
    "    def allSentences(self):\n",
    "        '''process the sentences: *0 times the original sentences, and \n",
    "            drop words with the prob based on each token's freq'''\n",
    "        if hasattr(self, '_allsentences') and self._allsentences:\n",
    "            return self._allsentences\n",
    "\n",
    "        sentences = self.sentences()\n",
    "        rejectProb = self.rejectProb()\n",
    "        tokens = self.tokens()\n",
    "        # NOTE: 先*30？？？根据拒绝率拒绝（下采样？\n",
    "        allsentences = [[w for w in s if 0 >= rejectProb[tokens[w]] or random.random() >= rejectProb[tokens[w]]] for s in sentences * 30]\n",
    "\n",
    "        allsentences = [s for s in allsentences if len(s) > 1]  # 只保留了至少有2个单词的句子\n",
    "\n",
    "        self._allsentences = allsentences\n",
    "        \n",
    "        return self._allsentences\n",
    "\n",
    "    def getRandomContext(self, C=5):\n",
    "        '''get random center word and its not NULL context words'''\n",
    "        allsent = self.allSentences()   # 获取了处理过的句子\n",
    "        sentID = random.randint(0, len(allsent) - 1)\n",
    "        sent = allsent[sentID]\n",
    "        wordID = random.randint(0, len(sent) - 1)\n",
    "\n",
    "        context = sent[max(0, wordID - C):wordID + C + 1]   # 开头结尾直接不算\n",
    "        # if wordID + 1 < len(sent):\n",
    "        #     context += sent[wordID + 1:min(len(sent), wordID + C + 1)]\n",
    "\n",
    "        centerword = sent[wordID]\n",
    "        # context = [w for w in context if w != centerword]   # 这样还是会把相同的词搞掉诶\n",
    "        context = sent[:wordID] + sent[wordID + 1:]\n",
    "\n",
    "        if len(context) > 0:\n",
    "            return centerword, context\n",
    "        else:\n",
    "            return self.getRandomContex(C)  # 如果发现生成的context word为空，则直接重新生成一遍\n",
    "\n",
    "    def sent_labels(self):\n",
    "        '''return the sent-label mapping'''\n",
    "        if hasattr(self, '_sent_labels') and self._sent_labels:\n",
    "            return self._sent_labels\n",
    "        \n",
    "        dictionary = dict()\n",
    "        phrases = 0\n",
    "        # with open(self.path + \"/dictionary.txt\", 'r') as f:\n",
    "        with open(self.path + \"/dictionary.txt\", 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line: continue   # 空行不做处理\n",
    "                splitted = line.split(\"|\")\n",
    "                dictionary[splitted[0].lower()] = int(splitted[1])  # ID lookup table?\n",
    "                phrases += 1\n",
    "        \n",
    "        labels = [0.0] * phrases\n",
    "        with open(self.path + '/sentiment_labels.txt', 'r') as f:\n",
    "            first = True\n",
    "            for line in f:\n",
    "                if first:\n",
    "                    first = False\n",
    "                    continue\n",
    "            \n",
    "                line = line.strip()\n",
    "                if not line: continue\n",
    "                splitted = line.split(\"|\")\n",
    "                labels[int(splitted[0])] = float(splitted[1])\n",
    "\n",
    "        sent_labels = [0.0] * self.numSentences()\n",
    "        sentences = self.sentences()\n",
    "        for i in range(self.numSentences()):\n",
    "            sentence = sentences[i]\n",
    "            full_sent = \" \".join(sentence).replace('-lrb-', '(').replace('-rrb-',')')   # ????\n",
    "            sent_labels[i] = labels[dictionary[full_sent]]\n",
    "\n",
    "        self._sent_labels = sent_labels\n",
    "        return self._sent_labels\n",
    "\n",
    "    def dataset_split(self):\n",
    "        '''divide the dataset into train:dev:test according to the txt'''\n",
    "        if hasattr(self, '_split') and self._split:\n",
    "            return self.dataset_split\n",
    "        \n",
    "        split = [[] for i in range(3)]\n",
    "        with open(self.path + \"/datasetSplit.txt\", 'r') as f:\n",
    "            first = True\n",
    "            for line in f:\n",
    "                if first:\n",
    "                    first = False\n",
    "                    continue\n",
    "                \n",
    "                splitted = line.strip().split(',')\n",
    "                split[int(splitted[1]) - 1] += [int(splitted[0]) - 1]\n",
    "            \n",
    "        self._split = split\n",
    "        return self._split\n",
    "\n",
    "    def getRandomTrainSentence(self):\n",
    "        '''randomly choose a traindata'''\n",
    "        split = self.dataset_split()\n",
    "        sentId = split[0][random.randint(0, len(split[0]) - 1)]\n",
    "        return self.sentences()[sentId], self.categorify(self.sent_labels()[sentId])\n",
    "\n",
    "    def categorify(self, label):\n",
    "        '''map real grade into categorical classes'''\n",
    "        if label <= 0.2:   return 0\n",
    "        elif label <= 0.4: return 1\n",
    "        elif label <= 0.6: return 2\n",
    "        elif label <= 0.8: return 3\n",
    "        else: return 4\n",
    "\n",
    "    def getDevSentences(self):\n",
    "        return self.getSplitSentences(2)\n",
    "    \n",
    "    def getTestSentences(self):\n",
    "        return self.getSplitSentences(1)\n",
    "    \n",
    "    def getTrainSentences(self):\n",
    "        return self.getSplitSentences(0)\n",
    "\n",
    "    def getSplitSentences(self, split=0):\n",
    "        '''return the splitted data'''\n",
    "        ds_split = self.dataset_split()\n",
    "        return [(self.sentences()[i], self.categorify(self.sent_labels()[i])) for i in ds_split[split]]\n",
    "    \n",
    "    def sampleTable(self):\n",
    "        '''first construct a table, then randomly choose from it'''\n",
    "        if hasattr(self, '_sampleTable') and self._sampleTable is not None:\n",
    "            return self._sampleTable\n",
    "\n",
    "        nTokens = len(self.tokens())\n",
    "        samplingFreq = np.zeros((nTokens,))\n",
    "        self.allSentences()\n",
    "        \n",
    "        for i in range(nTokens):\n",
    "            w = self._revtokens[i]\n",
    "            if w in self._tokenfreq:\n",
    "                freq = 1.0 * self._tokenfreq[w]\n",
    "                # reweigh\n",
    "                freq = freq ** 0.75\n",
    "            else:   # NOTE: 为什么会有不在self.tokens里的单词？\n",
    "                freq = 0.0\n",
    "            samplingFreq[i] = freq\n",
    "\n",
    "        samplingFreq /= np.sum(samplingFreq)\n",
    "        samplingFreq = np.cumsum(samplingFreq) * self.tablesize # CDF?\n",
    "\n",
    "        self._sampleTable = [0] * self.tablesize\n",
    "\n",
    "        j = 0\n",
    "        for i in range(self.tablesize):\n",
    "            while i > samplingFreq[j]:\n",
    "                j += 1\n",
    "            self._sampleTable[i] = j\n",
    "\n",
    "        # NOTE：就是原论文中提到的sample方法；先打表，然后再取对应数值\n",
    "        return self._sampleTable    \n",
    "    \n",
    "    def rejectProb(self):\n",
    "        '''assign the rejection prob to each unique token (with the same order)'''\n",
    "        if hasattr(self, '_rejectProb') and self._rejectProb is not None:\n",
    "            return self._rejectProb     # 为什么这里用了is not None\n",
    "\n",
    "        threshold = 1e-5 * self._wordcount\n",
    "\n",
    "        nTokens = len(self.tokens())\n",
    "        rejectProb = np.zeros((nTokens,))\n",
    "        for i in range(nTokens):\n",
    "            w = self._revtokens[i]\n",
    "            freq = 1.0 * self._tokenfreq[w]\n",
    "            # reweight  NOTE: 为什么不用那个3/4次方的式子？\n",
    "            rejectProb[i] = max(0, 1 - np.sqrt(threshold / freq))\n",
    "\n",
    "        self._rejectProb = rejectProb\n",
    "        return self._rejectProb\n",
    "\n",
    "    def sampleTokenIdx(self):\n",
    "        '''get a token index ranomly according to the negative drop out rate'''\n",
    "        # 注意一下tablesize到底是啥\n",
    "        return self.sampleTable()[random.randint(0, self.tablesize - 1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PARAMS_EVERY = 5000\n",
    "# path = ''\n",
    "path = \"../../models/C9dim10\"\n",
    "\n",
    "def load_saved_params():\n",
    "    '''helper function to loads previously saved params'''\n",
    "    st = 0\n",
    "    for f in glob.glob(path+\"saved_params_*.npy\"):   # 在当前文件夹下找到所有类似的文件名\n",
    "        iter = int(op.splitext(op.basename(f))[0].split(\"_\")[2])\n",
    "        if (iter > st): st = iter\n",
    "    \n",
    "    if st > 0:\n",
    "        params_file = path+\"saved_params_%d.npy\" % st\n",
    "        state_file = path+\"saved_state_%d.pickle\" % st\n",
    "        params = np.load(params_file,allow_pickle=True)\n",
    "        with open(state_file, \"rb\") as f:\n",
    "            state = pickle.load(f)\n",
    "        return st, params, state\n",
    "    else:\n",
    "        return st, None, None\n",
    "\n",
    "def save_params(iter, params):\n",
    "    params_file = path+\"saved_params_%d.npy\" % iter\n",
    "    np.save(params_file, params)\n",
    "    with open(path+\"saved_state_%d.pickle\" % iter, \"wb\") as f:\n",
    "        pickle.dump(random.getstate(), f)\n",
    "\n",
    "def sgd(f, x0, step, iterations, postprocessing=None, useSaved=False, PRINT_EVERY = 10):\n",
    "    '''Stochastic Gradient Descent\n",
    "       word2vec should be normalized to unit length\n",
    "    '''\n",
    "    ANNEAL_EVERY = 20000    # 没过这么多步，lr衰减一半\n",
    "\n",
    "    if useSaved:\n",
    "        start_iter, oldx, state = load_saved_params()\n",
    "        if start_iter > 0:\n",
    "            x0 = oldx\n",
    "            step *= 0.5 ** (start_iter / ANNEAL_EVERY)\n",
    "\n",
    "        if state:\n",
    "            random.setstate(state)\n",
    "\n",
    "    else:\n",
    "        start_iter = 0\n",
    "\n",
    "    x = x0\n",
    "\n",
    "    if not postprocessing:\n",
    "        postprocessing = lambda x: x\n",
    "\n",
    "    exploss = None\n",
    "\n",
    "    for iter in range(start_iter + 1, iterations + 1):\n",
    "        loss = None\n",
    "        loss, grad = f(x)\n",
    "        x -= step * grad\n",
    "\n",
    "        x = postprocessing(x)\n",
    "        if iter % PRINT_EVERY == 0 or iter == 100:\n",
    "            if not exploss: exploss = loss\n",
    "            else: exploss = .95 * exploss + .05 * loss\n",
    "            # NOTE: 只加上0.5%的新loss？\n",
    "\n",
    "            print(\"iter %d: %f\" % (iter, exploss))\n",
    "\n",
    "        if iter % SAVE_PARAMS_EVERY == 0 and useSaved:\n",
    "            save_params(iter, x)\n",
    "\n",
    "        if iter % ANNEAL_EVERY == 0:\n",
    "            step *= 0.5\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeRows(x):\n",
    "    '''row normalization function'''\n",
    "    assert len(x.shape) <= 2\n",
    "    x = x / np.sqrt(np.sum(np.power(x,2), axis=len(x.shape) -1 )).reshape(-1,1) + 1e-30\n",
    "    return x\n",
    "\n",
    "def softmax(x):\n",
    "    '''compute the softmax function for each row of the input x'''\n",
    "    assert len(x.shape) <= 2\n",
    "    y = np.exp(x - np.max(x, axis=len(x.shape) - 1, keepdims=True))\n",
    "    normalization = np.sum(y, axis=len(x.shape) - 1, keepdims=True)\n",
    "    return np.divide(y, normalization)\n",
    "\n",
    "def sigmoid(x):\n",
    "    '''compute the sigmoid function for the input'''\n",
    "    s = 1 / (1 + np.exp(-x))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmaxCostAndGradient(v_c, target, outputVectors, dataset):\n",
    "    '''所以center word vecs只更新当前center, context word vecs会全部更新'''\n",
    "    u_o = outputVectors[target, : ]     # the target outside word (1,M)\n",
    "    # v_c = predicted     # center word vector (1,M)\n",
    "    u_w = outputVectors # all context/outside words vectors (V,M)\n",
    "\n",
    "    p = softmax(np.dot(u_w,v_c))    # (V,M) dot (M,1) => (V,1)\n",
    "    cost = -np.log(p[target])       # target is the true outside word we want\n",
    "    gradCenterVec = -u_o + np.dot(p,u_w)    # (1,M) - (V,) dot (V,M) => (1,M)\n",
    "    p[target] -= 1      # deal with the u_o\n",
    "    gradOutsideVecs = np.outer(p, v_c)  # (V,) outer (1,M) => (V,M)\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return cost, gradCenterVec, gradOutsideVecs\n",
    "\n",
    "\n",
    "def negSamplingCostAndGradient(v_c, target, outputVectors, dataset, K=10):\n",
    "    \"\"\" Negative sampling cost function for word2vec models \"\"\"\n",
    "    \"\"\" Samples K indexes which are not the outsideWordIdx \"\"\"\n",
    "    u_o = outputVectors[target, : ]     # the target outside word (1,M)\n",
    "    # v_c = predicted     # center word vector (1,M)\n",
    "    \n",
    "    k_ind,_ = [],0\n",
    "    while _ < K:\n",
    "        new_ind = dataset.sampleTokenIdx()\n",
    "        if new_ind != target:\n",
    "            k_ind.append(new_ind)\n",
    "            _ += 1\n",
    "    \n",
    "\n",
    "    k_ind = np.array(k_ind)\n",
    "    u_k = outputVectors[k_ind]  # (K,M)\n",
    "\n",
    "    sigmoid_o = sigmoid(np.dot(u_o, v_c))   # (1,M) dot (M,) => scaler\n",
    "    sigmoid_k = sigmoid(-np.dot(u_k, v_c))  # (K,M) dot (M,) => (K,)\n",
    "    cost = - np.log(sigmoid_o) - np.log(sigmoid_k).sum()\n",
    "\n",
    "    sigmoid_o -= 1.0\n",
    "    sigmoid_k = 1.0 - sigmoid_k \n",
    "    gradCenterVec = np.dot(sigmoid_o, u_o) + np.dot(sigmoid_k, u_k) # c * (1,M) + (1,K) dot (K,M) =? (1,M)\n",
    "\n",
    "    gradOutsideVecs = np.zeros(outputVectors.shape, dtype=np.float32)\n",
    "    gradOutsideVecs[target] += np.dot(sigmoid_o, v_c)    # scaler dot (M,)\n",
    "\n",
    "    temp = np.outer(sigmoid_k, v_c)\n",
    "    for i in range(len(k_ind)):\n",
    "        gradOutsideVecs[k_ind[i]] += temp[i]\n",
    "\n",
    "    # NOTE: ???为什么用下面这行会过不了gradient的check？？\n",
    "    # gradOutsideVecs[k_ind] += np.outer(sigmoid_k, v_c)   # (1,K) outer (M,)\n",
    "    \n",
    "    return cost, gradCenterVec, gradOutsideVecs\n",
    "\n",
    "def skipgram(currentWord, C, contextWords, tokens, centerVectors, contextVectors,\n",
    "                dataset, word2vecCostAndGradient = softmaxCostAndGradient):\n",
    "    \"\"\" Skip-gram model in word2vec \"\"\"\n",
    "    cost, gradIn, gradOut = 0, np.zeros(centerVectors.shape), np.zeros(contextVectors.shape)\n",
    "    for word in contextWords:\n",
    "        cur_cost, gradCenterVec, gradOutsideVecs = word2vecCostAndGradient(centerVectors[tokens[currentWord]], tokens[word], contextVectors, dataset)\n",
    "        cost += cur_cost\n",
    "        gradIn[tokens[currentWord]] += gradCenterVec\n",
    "        gradOut += gradOutsideVecs\n",
    "\n",
    "    return cost, gradIn, gradOut\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_sgd_wrapper(word2vecModel, word2Ind, wordVectors, dataset,\n",
    "                         windowSize, \n",
    "                         word2vecCostAndGradient = softmaxCostAndGradient):\n",
    "    ''''''\n",
    "    batchsize = 50\n",
    "    loss = 0.0\n",
    "    grad = np.zeros(wordVectors.shape)\n",
    "    N = wordVectors.shape[0]\n",
    "    centerWordVectors = wordVectors[:N//2, :]\n",
    "    outsideVectors = wordVectors[N//2:, :]\n",
    "    for i in range(batchsize):\n",
    "        # NOTE：why window size 还是随机再选一遍？\n",
    "        windowSize1 = random.randint(1, windowSize)\n",
    "        centerWord, context = dataset.getRandomContext(windowSize1)\n",
    "\n",
    "        c, gin, gout = word2vecModel(\n",
    "            centerWord, windowSize1, context, word2Ind, centerWordVectors,\n",
    "            outsideVectors, dataset, word2vecCostAndGradient\n",
    "        )\n",
    "        loss += c / batchsize\n",
    "        grad[:N//2,:] += gin / batchsize\n",
    "        grad[N//2:, :] += gout /batchsize\n",
    "\n",
    "    return loss, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the random seed\n",
    "random.seed(314)\n",
    "dataset = StanfordSentiment()\n",
    "tokens = dataset.tokens()\n",
    "nWords = len(tokens)\n",
    "\n",
    "# setting ups\n",
    "dimVectors = 10\n",
    "C = 5\n",
    "\n",
    "# reset random seed again\n",
    "random.seed(31415)\n",
    "np.random.seed(9265)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 100: 28.058487\n",
      "iter 1000: 28.156913\n",
      "iter 2000: 28.344944\n",
      "iter 3000: 28.434980\n",
      "iter 4000: 28.169390\n",
      "iter 5000: 27.742499\n",
      "iter 6000: 27.382251\n",
      "iter 7000: 26.863641\n",
      "iter 8000: 26.351885\n",
      "iter 9000: 25.948282\n",
      "iter 10000: 25.440994\n",
      "iter 11000: 25.001461\n",
      "iter 12000: 24.427942\n",
      "iter 13000: 23.843573\n",
      "iter 14000: 23.453748\n",
      "iter 15000: 23.008417\n",
      "iter 16000: 22.504239\n",
      "iter 17000: 22.125073\n",
      "iter 18000: 21.707264\n",
      "iter 19000: 21.286340\n",
      "iter 20000: 20.923356\n",
      "iter 21000: 20.551213\n",
      "iter 22000: 20.203605\n",
      "iter 23000: 19.814393\n",
      "iter 24000: 19.613641\n",
      "iter 25000: 19.260300\n",
      "iter 26000: 18.894713\n",
      "iter 27000: 18.628947\n",
      "iter 28000: 18.349784\n",
      "iter 29000: 18.050502\n",
      "iter 30000: 17.818498\n",
      "iter 31000: 17.644882\n",
      "iter 32000: 17.488010\n",
      "iter 33000: 17.326663\n",
      "iter 34000: 17.193238\n",
      "iter 35000: 16.951853\n",
      "iter 36000: 16.912663\n",
      "iter 37000: 16.674723\n",
      "iter 38000: 16.486823\n",
      "iter 39000: 16.401870\n",
      "iter 40000: 16.276004\n",
      "sanity check: cost at convergence should be around or below 10\n",
      "training took 13380 seconds\n"
     ]
    }
   ],
   "source": [
    "# initialize word vectors\n",
    "wordVectors = np.concatenate(\n",
    "    ((np.random.rand(nWords, dimVectors) - 0.5) / dimVectors, \n",
    "      np.zeros((nWords, dimVectors))), axis=0).astype(np.float32)\n",
    "\n",
    "startTime = time.time()\n",
    "# sgd(f, x0, step, iterations, postprocessing=None, useSaved=False, PRINT_EVERY = 10)\n",
    "# wordVectors = sgd(\n",
    "#     lambda vec: word2vec_sgd_wrapper(skipgram, tokens, vec, dataset, C, negSamplingCostAndGradient),\n",
    "#     wordVectors, 0.3, 40000, None, True, PRINT_EVERY = 1000\n",
    "# )\n",
    "wordVectors = sgd(\n",
    "    lambda vec: word2vec_sgd_wrapper(skipgram, tokens, vec, dataset, C, negSamplingCostAndGradient),\n",
    "    wordVectors, 0.3, 40000, None, False, PRINT_EVERY = 1000\n",
    ")\n",
    "\n",
    "# normalize during training loses the notion of length, so dropped here\n",
    "\n",
    "print(\"sanity check: cost at convergence should be around or below 10\")\n",
    "print(\"training took %d seconds\" % (time.time() - startTime))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���\n",
      "���������ļ���\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 648x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "dark"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "which_iter = 40000\n",
    "params = np.load(\"saved_params_%d.npy\" % which_iter,allow_pickle=True)\n",
    "wordVectors = params\n",
    "\n",
    "# # concatenate the input and output vectors NOTE: ???\n",
    "# wordVectors = np.concatenate(\n",
    "#     (wordVectors[:nWords,:], wordVectors[nWords:,:]),\n",
    "#     axis=0)\n",
    "\n",
    "# visualizeWords = [\"the\", \"a\", \"an\", \",\", \".\", \"?\", \"!\", \"``\", \"''\", \"--\", \n",
    "# \t\"good\", \"great\", \"cool\", \"brilliant\", \"wonderful\", \"well\", \"amazing\",\n",
    "# \t\"worth\", \"sweet\", \"enjoyable\", \"boring\", \"bad\", \"waste\", \"dumb\", \n",
    "# \t\"annoying\"]\n",
    "\n",
    "visualizeWords = [\n",
    "    \"great\", \"cool\", \"brilliant\", \"wonderful\", \"well\", \"amazing\",\n",
    "    \"worth\", \"sweet\", \"enjoyable\", \"boring\", \"bad\", \"dumb\",\n",
    "    \"annoying\", \"female\", \"male\", \"queen\", \"king\", \"man\", \"woman\", \"rain\", \"snow\",\n",
    "    \"hail\", \"coffee\", \"tea\"]\n",
    "\n",
    "\n",
    "visualizeIdx = [tokens[word] for word in visualizeWords]\n",
    "visualizeVecs = wordVectors[visualizeIdx, :]\n",
    "temp = (visualizeVecs - np.mean(visualizeVecs, axis=0))\n",
    "covariance = 1.0 / len(visualizeIdx) * temp.T.dot(temp)\n",
    "U,S,V = np.linalg.svd(covariance)\n",
    "coord = temp.dot(U[:,0:2])\n",
    "\n",
    "# show the vectors!\n",
    "fig = plt.figure(figsize=(9,6))\n",
    "ax = fig.add_subplot(111)\n",
    "for i in range(len(visualizeWords)):\n",
    "    ax.text(coord[i,0], coord[i,1], visualizeWords[i],\n",
    "        bbox=dict(facecolor='green', alpha=0.1))\n",
    "\n",
    "ax.set_xlim((np.min(coord[:,0]), np.max(coord[:,0])))\n",
    "ax.set_ylim((np.min(coord[:,1]), np.max(coord[:,1])))\n",
    "ax.xaxis.label.set_color('w')\n",
    "ax.yaxis.label.set_color('w')\n",
    "ax.tick_params(axis='x', colors='w')\n",
    "ax.tick_params(axis='y', colors='w')\n",
    "ax.set_title('Word Embedding Space', color='w');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = ''\n",
    "path = \"D:\\\\universityWorks\\\\thirdYear\\\\Spring\\\\DISC-NLP\\\\pj3\\\\models/C8dim10\"\n",
    "\n",
    "# def load_saved_params():\n",
    "#     '''helper function to loads previously saved parameters\n",
    "#        plus continue to iterate\n",
    "#     '''\n",
    "#     st = 0\n",
    "#     for f in glob.glob(path+\"saved_params_*.npy\"):\n",
    "#         iter = int(op.splitext(op.basename(f))[0].split(\"_\")[2])\n",
    "#         print(iter)\n",
    "#         if (iter > st):\n",
    "#             st = iter\n",
    "            \n",
    "#     if st > 0:\n",
    "#         with open(path+\"saved_params_%d.npy\" % st, \"rb\") as f:\n",
    "#             params = pickle.load(f)\n",
    "#             state = pickle.load(f)\n",
    "#             # params = np.load(f)\n",
    "#             # state = pickle.load(open(\"saved_state_%d.pickle\" % st,'rb'))\n",
    "#         return st, params, state\n",
    "#     else:\n",
    "#         print(\"st <= 0\")\n",
    "#         return st, None, None\n",
    "\n",
    "path = \"../../models/C8dim10\"\n",
    "\n",
    "def load_saved_params():\n",
    "    '''helper function to loads previously saved params'''\n",
    "    st = 0\n",
    "    for f in glob.glob(path+\"saved_params_*.npy\"):   # 在当前文件夹下找到所有类似的文件名\n",
    "        iter = int(op.splitext(op.basename(f))[0].split(\"_\")[2])\n",
    "        if (iter > st): st = iter\n",
    "    \n",
    "    if st > 0:\n",
    "        params_file = path+\"saved_params_%d.npy\" % st\n",
    "        state_file = path+\"saved_state_%d.pickle\" % st\n",
    "        params = np.load(params_file,allow_pickle=True)\n",
    "        with open(state_file, \"rb\") as f:\n",
    "            state = pickle.load(f)\n",
    "        return st, params, state\n",
    "    else:\n",
    "        return st, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentenceFeature(tokens, wordVectors, sentence):\n",
    "    '''obtain the sentence feature for sentiment analysis\n",
    "       by averaging its word vectors\n",
    "    '''\n",
    "    index = [tokens[word] for word in sentence]\n",
    "    return np.mean(wordVectors[index, :], axis=0) \n",
    "\n",
    "def softmaxRegression(features, labels, weights, regularization = 0.0, nopredictions = False):\n",
    "    '''softmax regression; \n",
    "    input:\n",
    "        - features: feature vectors, each row is a feature vector\n",
    "        - labels: labels corresponding to the feature vectors\n",
    "        - weights: weights of the regressor\n",
    "        - regularization: L2 regularization constant\n",
    "\n",
    "    output:\n",
    "        - cost: cost of the regressor\n",
    "        - grad: gradient of the regressor cost with respect to its weights\n",
    "        - pred: label predictions of the regressor\n",
    "    '''\n",
    "    prob = softmax(features.dot(weights))\n",
    "    if len(features.shape) > 1:\n",
    "        N = features.shape[0]\n",
    "    else:\n",
    "        N = 1\n",
    "    \n",
    "    # TODO：确认一下，是否其实还是分类器？\n",
    "    cost = np.sum(-np.log(prob[range(N), labels])) / (N + 1e-30)    # 防止溢出\n",
    "    cost += 0.5 * regularization * np.sum(weights ** 2) # L2 norm\n",
    "    labels_onehot = np.zeros_like(prob)\n",
    "    labels_onehot[range(N), labels] = 1.0\n",
    "    grad = 1.0 / N * np.dot(features.T, prob - labels_onehot) + regularization * weights\n",
    "    pred = np.argmax(prob, axis=len(prob.shape) - 1)\n",
    "    if nopredictions:\n",
    "        return cost, grad\n",
    "    else:\n",
    "        return cost, grad, pred\n",
    "    \n",
    "def accuracy(y, yhat):\n",
    "    '''precison for classifier'''\n",
    "    assert(y.shape == yhat.shape)\n",
    "    return np.sum(y == yhat) * 100.0 / y.size   # y.size 返回数量？\n",
    "\n",
    "def softmax_wrapper(features, labels, weights, regularization = 0.0):\n",
    "    cost, grad, _ = softmaxRegression(features, labels, weights, regularization)\n",
    "    return cost, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'but in imax 3-d , the clich脙漏s disappear into the vertiginous perspectives opened up by the photography .'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18188/4131866262.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# Load the train set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mtrainset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetTrainSentences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0mnTrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mtrainFeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnTrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdimVectors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18188/3458679133.py\u001b[0m in \u001b[0;36mgetTrainSentences\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgetTrainSentences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetSplitSentences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgetSplitSentences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18188/3458679133.py\u001b[0m in \u001b[0;36mgetSplitSentences\u001b[1;34m(self, split)\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[1;34m'''return the splitted data'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m         \u001b[0mds_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategorify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msent_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mds_split\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msampleTable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18188/3458679133.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[1;34m'''return the splitted data'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m         \u001b[0mds_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategorify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msent_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mds_split\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msampleTable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18188/3458679133.py\u001b[0m in \u001b[0;36msent_labels\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    148\u001b[0m             \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m             \u001b[0mfull_sent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-lrb-'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'('\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-rrb-'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m')'\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# ????\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m             \u001b[0msent_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfull_sent\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sent_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msent_labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'but in imax 3-d , the clich脙漏s disappear into the vertiginous perspectives opened up by the photography .'"
     ]
    }
   ],
   "source": [
    "REGULARIZATION = [1e-8,1e-7,1e-6,1e-5,1e-4,1e-3,1e-2,1e-1,0.5]\n",
    "\n",
    "# Load the dataset\n",
    "dataset = StanfordSentiment()\n",
    "tokens = dataset.tokens()\n",
    "nWords = len(tokens)\n",
    "\n",
    "\n",
    "which_iter = 40000\n",
    "params = np.load(\"saved_params_%d.npy\" % which_iter,allow_pickle=True)\n",
    "wordVectors0 = params\n",
    "\n",
    "\n",
    "# Load the word vectors we trained earlier \n",
    "# _, wordVectors0, _ = load_saved_params()\n",
    "wordVectors = (wordVectors0[:nWords-2,:] + wordVectors0[nWords-2:,:])\n",
    "dimVectors = wordVectors.shape[1]\n",
    "\n",
    "# Load the train set\n",
    "trainset = dataset.getTrainSentences()\n",
    "nTrain = len(trainset)\n",
    "trainFeatures = np.zeros((nTrain, dimVectors))\n",
    "trainLabels = np.zeros((nTrain,), dtype=np.int32)\n",
    "for i in range(nTrain):\n",
    "    words, trainLabels[i] = trainset[i]\n",
    "    trainFeatures[i, :] = getSentenceFeature(tokens, wordVectors, words)\n",
    "\n",
    "# Prepare dev set features\n",
    "devset = dataset.getDevSentences()\n",
    "nDev = len(devset)\n",
    "devFeatures = np.zeros((nDev, dimVectors))\n",
    "devLabels = np.zeros((nDev,), dtype=np.int32)\n",
    "for i in range(nDev):\n",
    "    words, devLabels[i] = devset[i]\n",
    "    devFeatures[i, :] = getSentenceFeature(tokens, wordVectors, words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for reg=0.000000\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'trainFeatures' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18188/4023029431.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# We will do batch optimization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     weights = sgd(lambda weights: softmax_wrapper(trainFeatures, trainLabels, \n\u001b[0m\u001b[0;32m     14\u001b[0m         weights, regularization), weights, 3.0, 10000, PRINT_EVERY=100)\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18188/1229351591.py\u001b[0m in \u001b[0;36msgd\u001b[1;34m(f, x0, step, iterations, postprocessing, useSaved, PRINT_EVERY)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_iter\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterations\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18188/4023029431.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(weights)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# We will do batch optimization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     weights = sgd(lambda weights: softmax_wrapper(trainFeatures, trainLabels, \n\u001b[0m\u001b[0;32m     14\u001b[0m         weights, regularization), weights, 3.0, 10000, PRINT_EVERY=100)\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trainFeatures' is not defined"
     ]
    }
   ],
   "source": [
    "# Try our regularization parameters\n",
    "import time\n",
    "startTime=time.time()\n",
    "\n",
    "results = []\n",
    "for regularization in REGULARIZATION:\n",
    "    random.seed(3141)\n",
    "    np.random.seed(59265)\n",
    "    weights = np.random.randn(dimVectors, 5)\n",
    "    print(\"Training for reg=%f\" % regularization )\n",
    "\n",
    "    # We will do batch optimization\n",
    "    weights = sgd(lambda weights: softmax_wrapper(trainFeatures, trainLabels, \n",
    "        weights, regularization), weights, 3.0, 10000, PRINT_EVERY=100)\n",
    "\n",
    "    # Test on train set\n",
    "    _, _, pred = softmaxRegression(trainFeatures, trainLabels, weights)\n",
    "    trainAccuracy = accuracy(trainLabels, pred)\n",
    "    print(\"Train accuracy (%%): %f\" % trainAccuracy)\n",
    "\n",
    "    # Test on dev set\n",
    "    _, _, pred = softmaxRegression(devFeatures, devLabels, weights)\n",
    "    devAccuracy = accuracy(devLabels, pred)\n",
    "    print(\"Dev accuracy (%%): %f\" % devAccuracy)\n",
    "\n",
    "    # Save the results and weights\n",
    "    results.append({\n",
    "        \"reg\" : regularization, \n",
    "        \"weights\" : weights, \n",
    "        \"train\" : trainAccuracy, \n",
    "        \"dev\" : devAccuracy})\n",
    "    \n",
    "\n",
    "# Print the accuracies\n",
    "print(\" took %d seconds\" % (time.time() - startTime))\n",
    "print(\"\")\n",
    "print(\"=== Recap ===\")\n",
    "print(\"Reg\\t\\tTrain\\t\\tDev\")\n",
    "for result in results:\n",
    "    print(\"%E\\t%f\\t%f\" % (result[\"reg\"],result[\"train\"],result[\"dev\"]))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the best regularization parameters\n",
    "BEST_IND = np.argmax([result[\"dev\"] for result in results])\n",
    "BEST_REGULARIZATION = results[BEST_IND]['reg']\n",
    "BEST_WEIGHTS = results[BEST_IND]['weights']\n",
    "\n",
    "# Test your findings on the test set\n",
    "testset = dataset.getTestSentences()\n",
    "nTest = len(testset)\n",
    "testFeatures = np.zeros((nTest, dimVectors))\n",
    "testLabels = np.zeros((nTest,), dtype=np.int32)\n",
    "for i in range(nTest):\n",
    "    words, testLabels[i] = testset[i]\n",
    "    testFeatures[i, :] = getSentenceFeature(tokens, wordVectors, words)\n",
    "\n",
    "_, _, pred = softmaxRegression(testFeatures, testLabels, BEST_WEIGHTS)\n",
    "print(\"Best regularization value: %E\" % BEST_REGULARIZATION)\n",
    "print(\"Test accuracy (%%): %f\" % accuracy(testLabels, pred))\n",
    "\n",
    "# Make a plot of regularization vs accuracy\n",
    "plt.plot(REGULARIZATION, [x[\"train\"] for x in results])\n",
    "plt.plot(REGULARIZATION, [x[\"dev\"] for x in results])\n",
    "plt.xscale('log')\n",
    "plt.xlabel(\"regularization\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.legend(['train', 'dev'], loc='upper left')\n",
    "plt.savefig(\"reg_acc.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c8f9177a0acce8018d23d7772672ff7f2c1807cf103258a4b51e26a443b2e37b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
